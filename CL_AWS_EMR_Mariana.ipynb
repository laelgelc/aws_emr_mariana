{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"https://laelgelcpublic.s3.sa-east-1.amazonaws.com/lael_50_years_narrow_white.png.no_years.400px_96dpi.png\" width=\"300\" alt=\"LAEL 50 years logo\">\n",
    "<h3>APPLIED LINGUISTICS GRADUATE PROGRAMME (LAEL)</h3>\n",
    "</center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction solution for Mariana's research project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mariana's research project's provisional title is: *Discurso infodêmico xenofóbico e aporofóbico em torno dos venezuelanos migrantes e refugiados: um estudo multidimensional lexical baseado em corpus*\n",
    "\n",
    "The proposed solution is aimed at extracting X tweets from 'The Twitter Grab 2019 Corpus' (Internet Archive) dataset. We will consider the archives from year 2019 initially and then extend to the target period ranging from 2015 to 2023.\n",
    "\n",
    "The archives are going to be filtered by the tweet field 'entities.hashtags.text' for hashtags and by the field 'text' for expressions. Due to technical reasons, the analysis of **hashtags** is case-sensitive. Therefore, the most common case combinations must be coded. This restriction does not affect the **expressions**.\n",
    "\n",
    "**Hashtags**\n",
    "- chavista\n",
    "- FueraVenecos\n",
    "- migrantevenezolano\n",
    "- portugalvenezuela\n",
    "- prayforvenezuela\n",
    "- refugiadovenezolano\n",
    "- transperuzolano\n",
    "- veneca\n",
    "- veneco\n",
    "- Venecobardes\n",
    "- venezolana\n",
    "- venezolandia\n",
    "- venezolano\n",
    "- venezolanodemierda\n",
    "- venezolanoshijosdeputa\n",
    "- venezuela\n",
    "- venezuelaenlacalle\n",
    "- venezuelazo\n",
    "- venezuelazuela\n",
    "\n",
    "**Expressions**\n",
    "- Caraqueño\n",
    "- Chaveta veneco\n",
    "- Criollo veneco\n",
    "- Guarimbeiros\n",
    "- Parasitos venezolanos\n",
    "- Refugiados de la miseria venezuela\n",
    "- Sudaca venezolano\n",
    "- Venecao\n",
    "- Veneccio\n",
    "- Venepobre\n",
    "- Veneputas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data with an Amazon EMR Apache Spark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into an Amazon EMR EMR Apache Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the data source accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T20:54:48.057149Z",
     "iopub.status.busy": "2023-12-19T20:54:48.056907Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3dcd4a126db4c528c0a18f2d521d7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1144b934a61b4db7a26b32fc77762eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName('The Twitter Grab 2019 Corpus').getOrCreate()\n",
    "\n",
    "# Set the S3 bucket and folder paths\n",
    "source_bucket = 'gelctweets'\n",
    "year = '2019'\n",
    "month = '01'\n",
    "data_source = 's3://' + source_bucket + '/' + year + '_' + month + '/*/*/*/*.json'\n",
    "#data_source = 's3://' + source_bucket + '/' + year + '_' + month + '/01/00/29.json.bz2/*.json'\n",
    "\n",
    "# Read the JSONL files into a DataFrame\n",
    "tweets_spark_df = spark.read.json(data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first few rows of the DataFrame\n",
    "tweets_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the quantity of columns of the DataFrame\n",
    "len(tweets_spark_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the quantity of rows (tweets) of the DataFrame\n",
    "tweets_spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema of the DataFrame\n",
    "tweets_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, col, array_contains\n",
    "\n",
    "# Define the list of hashtags for DataFrame filtering\n",
    "hashtags = [\n",
    "    'chavista', \n",
    "    'Chavista', \n",
    "    'CHAVISTA', \n",
    "    'fueravenecos', \n",
    "    'Fueravenecos', \n",
    "    'FueraVenecos', \n",
    "    'FUERAVENECOS', \n",
    "    'migrantevenezolano', \n",
    "    'Migrantevenezolano', \n",
    "    'MigranteVenezolano', \n",
    "    'MIGRANTEVENEZOLANO', \n",
    "    'portugalvenezuela', \n",
    "    'Portugalvenezuela', \n",
    "    'PortugalVenezuela', \n",
    "    'PORTUGALVENEZUELA', \n",
    "    'prayforvenezuela', \n",
    "    'Prayforvenezuela', \n",
    "    'PrayForVenezuela', \n",
    "    'PRAYFORVENEZUELA', \n",
    "    'refugiadovenezolano', \n",
    "    'Refugiadovenezolano', \n",
    "    'RefugiadoVenezolano', \n",
    "    'REFUGIADOVENEZOLANO', \n",
    "    'transperuzolano', \n",
    "    'Transperuzolano', \n",
    "    'TRANSPERUZOLANO', \n",
    "    'veneca', \n",
    "    'Veneca', \n",
    "    'VENECA', \n",
    "    'veneco', \n",
    "    'Veneco', \n",
    "    'VENECO', \n",
    "    'venecobardes', \n",
    "    'Venecobardes', \n",
    "    'VENECOBARDES', \n",
    "    'venezolana', \n",
    "    'Venezolana', \n",
    "    'VENEZOLANA', \n",
    "    'venezolandia', \n",
    "    'Venezolandia', \n",
    "    'VENEZOLANDIA', \n",
    "    'venezolano', \n",
    "    'Venezolano', \n",
    "    'VENEZOLANO', \n",
    "    'venezolanodemierda', \n",
    "    'Venezolanodemierda', \n",
    "    'VenezolanoDeMierda', \n",
    "    'VENEZOLANODEMIERDA', \n",
    "    'venezolanoshijosdeputa', \n",
    "    'Venezolanoshijosdeputa', \n",
    "    'VenezolanosHijosDePuta', \n",
    "    'VENEZOLANOSHIJOSDEPUTA', \n",
    "    'venezuela', \n",
    "    'Venezuela', \n",
    "    'VENEZUELA', \n",
    "    'venezuelaenlacalle', \n",
    "    'Venezuelaenlacalle', \n",
    "    'VenezuelaEnLaCalle', \n",
    "    'VENEZUELAENLACALLE', \n",
    "    'venezuelazo', \n",
    "    'Venezuelazo', \n",
    "    'VENEZUELAZO', \n",
    "    'venezuelazuela', \n",
    "    'Venezuelazuela'\n",
    "]\n",
    "\n",
    "expressions = [\n",
    "    'caraqueño', \n",
    "    'chaveta veneco', \n",
    "    'criollo veneco', \n",
    "    'guarimbeiros', \n",
    "    'parasitos venezolanos', \n",
    "    'refugiados de la miseria venezuela', \n",
    "    'sudaca venezolano', \n",
    "    'venecao', \n",
    "    'veneccio', \n",
    "    'venepobre', \n",
    "    'veneputas'\n",
    "]\n",
    "\n",
    "# Create a filtered DataFrame\n",
    "filtered_tweets_spark_df = tweets_spark_df.filter(\n",
    "    array_contains('entities.hashtags.text', hashtags[0]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[1]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[2]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[3]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[4]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[5]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[6]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[7]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[8]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[9]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[10]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[11]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[12]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[13]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[14]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[15]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[16]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[17]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[18]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[19]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[20]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[21]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[22]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[23]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[24]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[25]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[26]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[27]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[28]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[29]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[30]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[31]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[32]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[33]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[34]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[35]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[36]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[37]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[38]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[39]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[40]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[41]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[42]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[43]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[44]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[45]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[46]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[47]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[48]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[49]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[50]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[51]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[52]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[53]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[54]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[55]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[56]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[57]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[58]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[59]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[60]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[61]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[62]) |\\\n",
    "    array_contains('entities.hashtags.text', hashtags[63]) |\\\n",
    "    lower(col('text')).contains(expressions[0]) |\\\n",
    "    lower(col('text')).contains(expressions[1]) |\\\n",
    "    lower(col('text')).contains(expressions[2]) |\\\n",
    "    lower(col('text')).contains(expressions[3]) |\\\n",
    "    lower(col('text')).contains(expressions[4]) |\\\n",
    "    lower(col('text')).contains(expressions[5]) |\\\n",
    "    lower(col('text')).contains(expressions[6]) |\\\n",
    "    lower(col('text')).contains(expressions[7]) |\\\n",
    "    lower(col('text')).contains(expressions[8]) |\\\n",
    "    lower(col('text')).contains(expressions[9]) |\\\n",
    "    lower(col('text')).contains(expressions[10])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first few rows of the DataFrame\n",
    "filtered_tweets_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the quantity of rows (tweets) of the DataFrame\n",
    "filtered_tweets_spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema of the DataFrame\n",
    "filtered_tweets_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the output path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to JSONL format\n",
    "#output_path = 's3://gelcawsemr/2019_01_01_00/filtered_tweets.jsonl'\n",
    "output_path = 's3://gelcawsemr/2019_01/filtered_tweets.jsonl'\n",
    "filtered_tweets_spark_df.write.mode('overwrite').json(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
